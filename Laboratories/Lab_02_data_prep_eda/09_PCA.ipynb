{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "![pca2d](PCA_2d.svg)\n",
    "![pca](pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Satisfaction Analysis\n",
    "\n",
    "A survey in order to evaluate 20 different healthcare structures. 200 customers have evaluated, with a 1-10 scale, each of six features of the service:\n",
    "\n",
    "1. Courtesy\n",
    "2. Clarity\n",
    "3. Competence\n",
    "4. Condition (of the structure)\n",
    "5. Promptness (of the service)\n",
    "6. Opening times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"csat.csv\")\n",
    "print(df2.head(10))\n",
    "\n",
    "df2.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df2.boxplot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset scaling and visualizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler2 = StandardScaler(copy=False) \n",
    "scaler2.fit(df2.astype(float)) # \n",
    "scaler2.transform(df2.astype(float))\n",
    "df2.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_scaled=pd.DataFrame(scaler2.transform(df2.astype(float))) \n",
    "df2_scaled.columns=df2.columns\n",
    "df2_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_scaled.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.apply(lambda s: df2.corrwith(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA fit\n",
    "from sklearn.decomposition import PCA\n",
    "# we can choose the number of components e.g. 10, the percentage of the total variance or set it to None (that means it automatically chooses the number of components)\n",
    "pca2 = PCA()\n",
    "pca2.fit(df2_scaled) #The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use the pca to transform the dataset\n",
    "df2_pca = pd.DataFrame(pca2.transform(df2_scaled))\n",
    "df2_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's analyse what happened\n",
    "#VISUALIZE The amount of variance explained by each of the 10 selected principal components.\n",
    "pd.DataFrame(pca2.explained_variance_).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZE The percentage of variance explained by each of the selected components.\n",
    "explained_var=pd.DataFrame(pca2.explained_variance_ratio_).transpose()\n",
    "explained_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZE The cumulative percentage of explained variance\n",
    "cum_explained_var=np.cumsum(pca2.explained_variance_ratio_)\n",
    "pd.DataFrame(cum_explained_var).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "ax = sns.barplot( data=explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca2.components_,index=['PC1','PC2','PC3','PC4','PC5','PC6'],columns=df2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.grid()\n",
    "\n",
    "#Call the function. Use only the 2 PCs.\n",
    "myplot(pca2.transform(df2_scaled)[:,0:2],np.transpose(pca2.components_[0:2, :]), df2.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st component:\n",
    "\n",
    "The variables\n",
    "\n",
    "   - Condition\n",
    "   - Promptness\n",
    "   - Opening-times\n",
    "\n",
    "show a high correlation with the first component. This component can be summarized as an index of the **structure’s performances**\n",
    "\n",
    "### 2nd component:\n",
    "\n",
    "The variables\n",
    "   - Courtesy\n",
    "   - Clarity\n",
    "   - Competence\n",
    "\n",
    "show a high correlation with the second component. \n",
    "This component can be summarized as an index of the **personnel’s performance**\n",
    "\n",
    "**Notice that the Principal Components have negative values in the variables that they explain.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_pca.columns=['PC1','PC2','PC3','PC4','PC5','PC6']\n",
    "df2_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=sns.scatterplot(x=\"PC1\", y=\"PC2\",\n",
    "              alpha=.3, \n",
    "              hue=\"PC6\", legend=False,\n",
    "              data=df2_pca);\n",
    "\n",
    "# add annotations one by one with a loop\n",
    "for line in range(0,df2_pca.shape[0]):\n",
    "     p1.text(df2_pca.PC1[line], df2_pca.PC2[line], line, horizontalalignment='left', size='medium', color='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that \n",
    " - centers 18,11 has a GOOD infrastructure but a BAD service\n",
    " - the group near 1,4,6 has BAD infrastructure but GOOD service quality \n",
    " - the group 0,3,2,9 has GOOD infrastructure and service\n",
    " - center 17 has BAD infrastructure and service!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer wisconsin (diagnostic) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#upload a toy datasets from scikit-learn\n",
    "#sklearn comes with a few small standard datasets that do not require to download any file from some external website\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer() #The breast cancer dataset is a classic and very easy binary classification dataset.\n",
    "\n",
    "#create the dataframe\n",
    "dataset_df = pd.DataFrame(dataset.data)\n",
    "columns = dataset.feature_names\n",
    "dataset_df.columns = columns\n",
    "\n",
    "print(dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset visualization tools\n",
    "%matplotlib inline\n",
    "dataset_df.boxplot()\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset scaling and visualizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(copy=False) #or alternatively use MinMaxScaler\n",
    "scaler.fit(dataset_df) \n",
    "scaler.transform(dataset_df) \n",
    "dataset_df.boxplot()\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = dataset_df.loc[:,'texture error']\n",
    "y = dataset_df.loc[:,'worst symmetry']\n",
    "\n",
    "\n",
    "plt.scatter(x, y,alpha=0.2,c=dataset.target )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "df_sample = dataset_df.copy()\n",
    "df_sample = df_sample.iloc[:,:8]\n",
    "df_sample['target']=dataset.target\n",
    "sns.pairplot(df_sample, hue='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA fit\n",
    "from sklearn.decomposition import PCA\n",
    "# we can choose the number of components e.g. 10, the percentage of the total variance or set it to None (that means it automatically chooses the number of components)\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(dataset_df) #The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use the pca to transform the dataset\n",
    "x_pca = pca.transform(dataset_df)\n",
    "print(\"Dataset shape before PCA: \", dataset_df.shape)\n",
    "print(\"Dataset shape after PCA: \", x_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's analyse what happened\n",
    "#VISUALIZE The amount of variance explained by each of the 10 selected principal components.\n",
    "pd.DataFrame(pca.explained_variance_).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZE The percentage of variance explained by each of the selected components.\n",
    "explained_var=pd.DataFrame(pca.explained_variance_ratio_).transpose()\n",
    "explained_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot( data=explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZE The cumulative percentage of explained variance\n",
    "cum_explained_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "pd.DataFrame(cum_explained_var).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINT the total percentage of explained variance \n",
    "print(cum_explained_var[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHOOSING THE NUMBER OF COMPONENTS - we can plot the cumulative percentage of explained variance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cum_explained_var)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This previous curve quantifies how much of the total, 30-dimensional variance is contained within the first 10 components. \n",
    "For example, we see that the first 4 components contain approximately 79% of the variance, \n",
    "while you need around 6 components to describe close to 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_,index=['pc1', 'pc2','pc3','pc4','pc5', 'pc6','pc7','pc8','pc9','pc10'],columns=dataset_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let see the coordinates of the data in the PCA \n",
    "principalDf = pd.DataFrame(data = x_pca\n",
    "             , columns = ['pc1', 'pc2','pc3','pc4','pc5', 'pc6','pc7','pc8','pc9','pc10'])\n",
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf['target']=dataset.target\n",
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data in the first PCA \n",
    "sns.scatterplot(x=\"pc1\",y=[0]*(principalDf['target'].size),\n",
    "              hue=\"target\", alpha=.2,\n",
    "              data =principalDf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data in the first two PCA \n",
    "sns.scatterplot(x=\"pc1\", y=\"pc2\",\n",
    "              hue=\"target\", alpha=.3,\n",
    "              data=principalDf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data in the first three PCA \n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(principalDf['pc1'], principalDf['pc2'],principalDf['pc3'], c=principalDf['target'], s=40)\n",
    "ax.view_init(60, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A PART FROM EXPLICITLY CHOOSE THE NUMBER OF PRINCIPAL COMPONENTS, YOU CAN RESORT TO SOME AUTOMATIC TOOLS SUCH AS:\n",
    "\n",
    "#(1) You can leave the pca implementation of sklearn to choose the number of components by using:\n",
    "    #Set n_components == 'mle' and svd_solver == 'full' and Minka’s MLE is used to guess the dimension. \n",
    "    \n",
    "pca = PCA(n_components='mle',svd_solver='full') \n",
    "pca.fit(dataset_df)\n",
    "pca.n_components_ \n",
    "#and then transform the dataset as we have already seen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use the pca to transform the dataset\n",
    "x_pca = pca.transform(dataset_df)\n",
    "print(\"Dataset shape before PCA: \", dataset_df.shape)\n",
    "print(\"Dataset shape after PCA: \", x_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR (2) you can ask for the components able to explain a certain percentage of variance by using:\n",
    "    #Set 0 < n_components < 1 and svd_solver == 'full' to select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.\n",
    "\n",
    "pca = PCA(n_components=0.9,svd_solver='full') \n",
    "pca.fit(dataset_df)\n",
    "pca.n_components_ \n",
    "#and then transform the dataset as we have already seen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use the pca to transform the dataset\n",
    "x_pca = pca.transform(dataset_df)\n",
    "print(\"Dataset shape before PCA: \", dataset_df.shape)\n",
    "print(\"Dataset shape after PCA: \", x_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mtcars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV mtcars\n",
    "cars = pd.read_csv('mtcars.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The car types are a mix that includes sedans (Datsun, Ford, Honda,…), luxury sedans (Mercedes, Cadellac,..), muscle cars (Javelin, Challenger, Camaro…) and high-end sports cars (Porsche, Lotus, Maserati, Ferrari…)\n",
    "\n",
    "- \tmpg \tMiles/US Gallon \tmpg is the determinant of fuel efficiency\n",
    "- \tcyl \tNumber of cylinders \tData includes vehicles with 4,6,8 cylinder engines.\n",
    "- \tdisp \tDisplacement (cu.in.) \tDisplacement measures overall volume in the engine as a factor of cylinder circumfrance, depth and total number of cylinders. This metric gives a good proxy for the total amount of power the engine can generate.\n",
    "- \thp \tGross horsepower \tGross horsepower measures the theoretical output of an engine’s power output\n",
    "- \tdrat \tRear axle ratio \tThe rear axle gear ratio indicates the number of turns of the drive shaft for every one rotation of the wheel axle. \n",
    "-   wt      weigth of the car \n",
    "- \tqsec \t1/4 mile time \tA performance measure, primarily of acceleration. Fastest time to travel 1/4 mile from standstill (in seconds).\n",
    "- \tvs \tV/S \tBinary variable signaling the engine cylinder configuration a V-shape (vs=0) or Straight Line (vs=1). V==0 and S==1. \n",
    "- \tam \tTransmission Type \tA binary variable signaling whether vehicle has automatic (am=0) or manual (am=1) transmission configuration.\n",
    "- \tgear \tNumber of forward gears \tNumber of gears in the transmission.\n",
    "- \tcarb \tNumber of carburetors \tThe number of carburetor barrels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset scaling and visualizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler3 = StandardScaler(copy=False) #or alternatively use MinMaxScaler\n",
    "scaler3.fit(cars.astype(float)) \n",
    "df_cars=pd.DataFrame(scaler3.transform(cars.astype(float))) \n",
    "df_cars.columns=cars.columns\n",
    "df_cars.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca3 = PCA()\n",
    "pca3.fit(df_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained = pd.DataFrame(pca3.explained_variance_ratio_).transpose()\n",
    "sns.barplot(data=explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca3.components_,columns=cars.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## pd.DataFrame(pca3.components_,columns=cars.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_pca = pd.DataFrame(pca3.transform(df_cars),columns = ['pc1', 'pc2','pc3','pc4','pc5', 'pc6','pc7','pc8','pc9','pc10','pc11']\n",
    "                        ,index=cars.index.values)\n",
    "cars_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2=sns.scatterplot(x=\"pc1\", y=\"pc2\",\n",
    "              alpha=.3,\n",
    "              data=cars_pca);\n",
    "# add annotations one by one with a loop\n",
    "for line in range(0,cars_pca.shape[0]):\n",
    "     p2.text(cars_pca.pc1[line], cars_pca.pc2[line], cars_pca.index[line], horizontalalignment='left', size='medium', color='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataframe\n",
    "iris_df = pd.DataFrame(iris.data)\n",
    "iris_df.columns = iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "iris_scaler = StandardScaler(copy=False) #or alternatively use MinMaxScaler\n",
    "iris_scaler.fit(iris_df) # \n",
    "iris_scaler.transform(iris_df)\n",
    "iris_df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA fit\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZE The percentage of variance explained by each of the selected components.\n",
    "pd.DataFrame(pca.explained_variance_ratio_).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained = pd.DataFrame(pca.explained_variance_ratio_).transpose()\n",
    "sns.barplot(data=explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_,columns=iris_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let see the coordinates of the data in the PCA \n",
    "iris_pca = pd.DataFrame(pca.transform(iris_df),columns = ['pc1', 'pc2','pc3','pc4']\n",
    "                        ,index=iris_df.index.values)\n",
    "iris_pca \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data in the first PCA \n",
    "sns.scatterplot(x=\"pc1\", y=0,\n",
    "              hue=iris['target'], alpha=.8,\n",
    "              data=iris_pca,\n",
    "               palette=\"deep\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data in the first two PCA \n",
    "sns.scatterplot(x=\"pc1\", y=\"pc2\",\n",
    "              hue=iris['target'], alpha=.3,\n",
    "              data=iris_pca);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
